\documentclass[10pt]{article}
\usepackage{multicol}
\usepackage{times}
\usepackage{graphicx}
\usepackage{mathtools}

\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}

\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}


\begin{document}
\nocite{Gz:5}
\nocite{Gz:6}
\nocite{Gz:7}
\nocite{Gz:8}
\nocite{Gz:9}
\nocite{Gz:10}
\nocite{Gz:11}
\nocite{Gz:12}
\title{A Revised Implementation of the GRAPH/Z Graph Processing System}

\author{
  Moudgalya, Shreyas\\
  \texttt{smoudgal@hawk.iit.edu}
}

\maketitle

\begin{abstract}
  The emerging applications in big data science and social increasing demands on large-scale graph-structured processing has led to the development of several graph-parallel abstractions including GRAPH/Z. However, in the case of using ZHT, a scalable distributed key-value store as building block of GRAPH/Z, the distribution is forced to use hash-based (random) partitioning which has potentially impaired their locality. The goal of this proposal would be to develop a graph partitioning scheme that can be used in the loading process of GRAPH/Z, a scalable graph processing system, to overcome several problems of its performance.
\end{abstract}

\begin{multicols}{2} 
  \section{Background Information}
  Graph partition problems fall under the category of NP-hard problems. Solutions to these problems are generally derived using heuristics and approximation algorithms. However, uniform graph partitioning or a balanced graph partition problem can be shown to be NP-complete to approximate within any finite factor.

  There have been a lot of work to handle big datasets of both commercial and scientific applications, including work flow systems, data streaming management systems and graph databases. These systems typically save the data in distributed file systems(such as Hadoop HDFS and FusionFS), SQL databases(such as Oracle and DB2), and NoSQL database(such as Cassandra and ZHT).

  Graph related query is tremendously slow on the traditional relational database, which makes it even more challenging to fully reveal and utilize the scientific and commercial value from the continuously increasing graph data sets. An ideal solution for this problem is to replace the traditional data infrastructure with a graph-centric model, including storage and computing, thus to better serve graph-based applications in terms of performance and programmability.

  However, since natural graphs are difficult to partition, the old GRAPH/Z system used ZHT’s hash function to physically distribute all the vertices. As a result, vertices and edges, which are the basic elements of graphs, are spread around different nodes. Due to the random partitioning scheme, when each vertex needs to get access to its edge list, it will statistically communicate to another node and make the communication cost become tremendously higher. Therefore, using hash function to partition the graph data can be considered as a worst case, which is determined by the characteristic of graph structure and the hash function. The original use case of hashing partition is dispersing the data to avoid hot spot, but it doesn’t help in the case of graph processing system.

  
  \section{Problem Statement}
  The Design and Implementation a Partitioning Scheme that works well on the large graph datasets on Graph/Z. The design and implementation of the algorithm must implement a fast, balanced and lightweight graph partition scheme based on graph traversal.
  
  \section{Related Work}
  Various graph partitioning algorithms exist. Since graph partitioning is a NP-hard problem, practical solutions are based on heuristics. There are two broad categories of methods used. They are local and global. Well known local methods are the Kernighan–Lin algorithm, and Fiduccia-Mattheyses algorithms, which were the first effective 2-way cuts by local search strategies. Their major drawback is the arbitrary initial partitioning of the vertex set, which can affect the final solution quality. Other Global approaches rely on properties of the entire graph and do not rely on an arbitrary initial partition. The most common example is spectral partitioning, where a partition is derived from the spectrum of the adjacency matrix.\\
  A multilevel graph partitioning algorithm works by applying one or more stages. Each stage reduces the size of the graph by collapsing vertices and edges, partitions the smaller graph, then maps back and refines this partition of the original graph. A wide variety of partitioning and refinement methods can be applied within the overall multi-level scheme. In many cases, this approach can give both fast execution times and very high quality results. One widely used example of such an approach is METIS, a graph partitioner, and hMETIS, the corresponding partitioner for hypergraphs.
  
  \section{Proposed Solution}
  Our main work to solve our problems of scalability is to backtrack and try to achieve good scaling on a single node. We believe that the problem with GRAPH/Z lies in bad data locality with ZHT. Our main goal is to achieve some level of competitiveness against GraphLab on a single node, eventually expanding to multi-node scaling through ZHT or another distributed datastore. We will measure performance from weak scaling with larger datasets, and profiling tools such as valgrind/callgrind. This main method will also apply to the implementation of a graph partitioning algorithm.
  
  \section{Evaluation}
 
  We will use a graph partitioning algorithm defined as:\\
  Given a graph $G = (V,E)$ and a a set of partitions, $P$, an output partition $V = v_0 \cup V_1 \cup V_2 \cup \ldots \cup V_{p-1}$ such that
  \begin{enumerate}
  \item ${V_i}$ are disjoint $\Rightarrow V_i \cap V_i = \emptyset$
  \item ${V_i}$ are roughly balanced $\Rightarrow |V_i| ~ |V_i|$
  \item let $E_{cut} \equiv {(u,v) | u \in V_i, v \in V_i, i \neq j}$\\
    Minimize $|E_{cut}|$
  \end{enumerate}
Results will come from from load balancing tests with other partitioning algorithms such as the Dynamic Cut-Cluster Algorithm, EvoPartition Algorithm, and the Kernighan–Lin Algorithm will be used for comparison. We would also profile the running time of the algorithms based on increase in graph vertices and increase in graph edges.
  \section{Deliverables}
  We will test the data on an  instance of AWS EC2 using the dataset from Stanford Network Analysis Project.
  
   \section{Conclusion}
   [placeholder]
   \bibliographystyle{plain}
   \bibliography{partition}
\end{multicols}
\end{document}
